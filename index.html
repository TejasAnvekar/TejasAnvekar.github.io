<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Tejas Anvekar</title> <meta name="author" content="Tejas Anvekar"> <meta name="description" content=""> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?5bd250fdc66fe9788afc2e9257226da0"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?e3a7352ea63996f57fd2f1dbd16a81e5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8E%93&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tejasanvekar.github.io/"> <style>@font-face{font-family:'Samarkan';src:url('assets/css/SAMARN__.woff') format('woff')}</style> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span style="font-family: Samarkan; color: #0076df">Tejas</span> <span style="font-family: Samarkan; font-weight: lighter;">Anvekar <sup><img src="assets/img/icons/india.png" alt="Flag" style="height: 25px;"></sup> </span> </h1> <p class="desc"> Computer Vision | <span id="yellow-highlight-2">3D Geometry</span> | Continual Learning ‚Ä¢ learn ‚áÑ imagine ‚áÑ manifest </p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/picture-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/picture-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/picture-1400.webp"></source> <img src="/assets/img/picture.jpg?4105064cb3ad38897c06676f0d3d0240" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="picture.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="more-info"> <a href="mailto:anvekartejas@gmail.com" class="icon"> <img src="assets/img/icons/gmail.png" alt="email"> </a> <a href="https://www.linkedin.com/in/tejas-anvekar-b88b461a4/?originalSubdomain=in" class="icon" rel="external nofollow noopener" target="_blank"> <img src="assets/img/icons/linkedin.png" alt="linkedin"> </a> <a href="https://twitter.com/anvekar_tejas" class="icon" rel="external nofollow noopener" target="_blank"> <img src="assets/img/icons/twitter.png" alt="Twitter"> </a> <a href="https://scholar.google.com/citations?user=nmHPl6QAAAAJ&amp;hl=en&amp;oi=ao" class="icon" rel="external nofollow noopener" target="_blank"> <img src="assets/img/icons/Google_Scholar_logo.svg.png" alt="scholar"> </a> <a href="assets/pdf/TejasAnvekar_Resume_v7.pdf" class="icon"> <img src="assets/img/icons/cv.png" alt="CV"> </a> </div> </div> <div style="text-align: justify; text-justify: inter-word;" class="clearfix"> <h4> <span class="wave">üëãüèæ</span>Hi! I'm Tejas!</h4> <p>currently working as <span id="yellow-highlight-1">Research Assistant</span> at <a href="https://www.kletech.ac.in/" rel="external nofollow noopener" target="_blank">KLE Technological University</a>, collaborating with <strong><a href="https://kletech.irins.org/profile/159972#other_information_panel" rel="external nofollow noopener" target="_blank">Prof. Uma Mudenagudi</a></strong> @ CoE Visual Intelligence <strong><a href="https://www.kletech.ac.in/research-innovation/research-centres/cevi" rel="external nofollow noopener" target="_blank">(CEVI)</a></strong>.</p> <p>My research focus spans the domains of <span id="purple-highlight-1"><strong>3D Comupter Vision</strong></span>, <strong>Self-Supervised Learning</strong>, and <strong>Continual Learning</strong>, with the penultimate aim of bestowing robots with a <span id="purple-highlight-2">human-like perception</span>.</p> <p>Beyond by academic endeavors, I serve as an external collaborator to distinguised scholars including <strong><a href="https://www.eecis.udel.edu/~chandra/" rel="external nofollow noopener" target="_blank">Prof. Chandra Kambhamettu</a></strong> @ <a href="https://bigdatavision.org/" rel="external nofollow noopener" target="_blank">VIMS Lab</a> | <span id="blue-highlight-1">University of Delaware</span> and <strong><a href="https://denabazazian.github.io/" rel="external nofollow noopener" target="_blank">Dr. Dena Bazazian</a></strong> @ <span id="blue-highlight-2">University of Plymouth</span>.</p> <p>I did my Bachelor‚Äôs degree in Electronics and Communication at <a href="https://www.kletech.ac.in/" rel="external nofollow noopener" target="_blank">KLE Technological University</a>, India where I was advised by <strong><a href="https://kletech.irins.org/profile/159972#other_information_panel" rel="external nofollow noopener" target="_blank">Prof. Uma Mudenagudi</a></strong> and worked on Self-Supervised geometric methods for a DST funded project to Categorize Crowdsourced data.</p> </div> <hr> <h2 class="font-weight"><a href="/news/" style="color: inherit;">News</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 40vh; overflow-y: scroll;"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Nov 1, 2023</th> <td> Our poster <strong><em>‚ÄúNovel Class Discovery for Representation of Real-World Heritage Data as Neural Radiance Fields‚Äù</em></strong> is accepted @ <strong><a href="https://aaai.org/aaai-conference/student-abstract-and-poster-program-call-for-proposals/" rel="external nofollow noopener" target="_blank">AAAI 2024</a></strong> </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 15, 2023</th> <td> Our first Dataset: <strong><em>‚ÄúA Benchmark Grocery Dataset of Realworld Point Clouds from Single View‚Äù</em></strong> in collaboration with <strong>Prof. Chandra Kambhamettu</strong>, Director of <a href="https://bigdatavision.org/" rel="external nofollow noopener" target="_blank">VIMS lab</a> | <a href="https://www.udel.edu/" rel="external nofollow noopener" target="_blank">University of Delaware</a>, is accepted @ IEEE <strong><a href="https://3dvconf.github.io/2024/" rel="external nofollow noopener" target="_blank">3DV 2024</a></strong> </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 21, 2023</th> <td> Two papers accepted @ <a href="https://iccv2023.thecvf.com/" rel="external nofollow noopener" target="_blank">ICCV 2023</a> workshop | <strong><a href="https://www.cvl.iis.u-tokyo.ac.jp/e-Heritage2023/index.php?id=workshop-program" rel="external nofollow noopener" target="_blank">e-heritage</a></strong>!! <ol> <li> <strong><em>‚Äú<a href="https://openaccess.thecvf.com/content/ICCV2023W/e-Heritage/papers/Kumbar_ASUR3D_Arbitrary_Scale_Upsampling_and_Refinement_of_3D_Point_Clouds_ICCVW_2023_paper.pdf" rel="external nofollow noopener" target="_blank">ASUR3D</a>: Arbitrary Scale Upsampling and Refinement of 3D Point Clouds Using Local Occupancy Fields‚Äù</em></strong> </li> <li> <strong><em>‚Äú<a href="https://openaccess.thecvf.com/content/ICCV2023W/e-Heritage/papers/Tabib_DeFi_Detection_and_Filling_of_Holes_in_Point_Clouds_Towards_ICCVW_2023_paper.pdf" rel="external nofollow noopener" target="_blank">DeFi</a>: Detection and Filling of Holes in Point Clouds Towards Restoration of Digitized Cultural Heritage Models‚Äù</em></strong> </li> </ol> </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 21, 2023</th> <td> Our paper <strong><em>‚Äú<a href="https://openaccess.thecvf.com/content/ICCV2023W/WiCV/papers/Kumbar_TP-NoDe_Topology-Aware_Progressive_Noising_and_Denoising_of_Point_Clouds_Towards_ICCVW_2023_paper.pdf" rel="external nofollow noopener" target="_blank">TP-NoDe</a>: Topology-Aware Progressive Noising and Denoising of Point Clouds Towards Upsampling‚Äù</em></strong> is accepted @ <strong><a href="https://sites.google.com/view/wicviccv2023/program/accepted-papers?authuser=0" rel="external nofollow noopener" target="_blank">WiCV</a></strong> | <a href="https://iccv2023.thecvf.com/" rel="external nofollow noopener" target="_blank">ICCV 2023</a> workshop. </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 28, 2023</th> <td> My work: <strong><em>‚Äú<a href="https://openaccess.thecvf.com/content/CVPR2023W/DLGC/papers/Anvekar_GPr-Net_Geometric_Prototypical_Network_for_Point_Cloud_Few-Shot_Learning_CVPRW_2023_paper.pdf" rel="external nofollow noopener" target="_blank">GPr-Net</a>: Geometric Prototypical Network for Point Cloud Few-Shot Learning‚Äù</em></strong> in collaboration with <strong><a href="https://denabazazian.github.io/" rel="external nofollow noopener" target="_blank">Dr. Dena Bazazian</a></strong>, Director of Studies | <a href="https://plymouth.ac.uk/" rel="external nofollow noopener" target="_blank">University of Plymouth</a>, is accepted @ <strong><a href="https://sites.google.com/view/dlgc-workshop-cvpr2023" rel="external nofollow noopener" target="_blank">DLGC</a></strong> | <a href="https://cvpr2023.thecvf.com/" rel="external nofollow noopener" target="_blank">CVPR 2023</a> workshop. </td> </tr> </table> </div> </div> <hr> <h2 class="font-weight"><a href="/publications/" style="color: inherit;">Selected Publications</a></h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/GPr-Net.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/GPr-Net.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/GPr-Net.gif-1400.webp"></source> <img src="/assets/img/publication_preview/GPr-Net.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="GPr-Net.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <abbr class="badge" style="background-color: #0076df;">CVPR-W</abbr> </div> <div id="Anvekar_2023_CVPR" class="col-sm-8"> <div class="title">GPr-Net: Geometric Prototypical Network for Point Cloud Few-Shot Learning</div> <div class="author"> <em><b>Tejas Anvekar</b></em>,¬†and¬†Dena Bazazian</div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</em>, Jun 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a href="http://arxiv.org/abs/2304.06007" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://openaccess.thecvf.com/content/CVPR2023W/DLGC/papers/Anvekar_GPr-Net_Geometric_Prototypical_Network_for_Point_Cloud_Few-Shot_Learning_CVPRW_2023_paper.pdf" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/TejasAnvekar/GPr-Net" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://drive.google.com/file/d/11I5iM36nXg8XnLRWHXfOIBdK-oy54RDi/view" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>In the realm of 3D-computer vision applications, point cloud few-shot learning plays a critical role. However, it poses an arduous challenge due to the sparsity, irregularity, and unordered nature of the data. Current methods rely on complex local geometric extraction techniques such as convolution, graph, and attention mechanisms, along with extensive data-driven pre-training tasks. These approaches contradict the fundamental goal of few-shot learning, which is to facilitate efficient learning. To address this issue, we propose GPr-Net (Geometric Prototypical Network), a lightweight and computationally efficient geometric prototypical network that captures the intrinsic topology of point clouds and achieves superior performance. Our proposed method, IGI++(Intrinsic Geometry Interpreter++) employs vector-based hand-crafted intrinsic geometry interpreters and Laplace vectors to extract and evaluate point cloud morphology, resulting in improved representations for FSL (Few-Shot Learning). Additionally, Laplace vectors enable the extraction of valuable features from point clouds with fewer points. To tackle the distribution drift challenge in few-shot metric learning, we leverage hyperbolic space and demonstrate that our approach handles intra and inter-class variance better than existing point cloud few-shot learning methods. Experimental results on the ModelNet40 dataset show that GPr-Net outperforms state-of-the-art methods in few-shot learning on point clouds, achieving utmost computational efficiency that is 170x better than all existing works. The code is publicly available at https://github. com/TejasAnvekar/GPr-Net.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Anvekar_2023_CVPR</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Anvekar, Tejas and Bazazian, Dena}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{GPr-Net: Geometric Prototypical Network for Point Cloud Few-Shot Learning}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4178-4187}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/VG-VAE.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/VG-VAE.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/VG-VAE.gif-1400.webp"></source> <img src="/assets/img/publication_preview/VG-VAE.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="VG-VAE.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <abbr class="badge" style="background-color: #0076df;">CVPR-W</abbr> </div> <div id="Anvekar_2022_CVPR" class="col-sm-8"> <div class="title">VG-VAE: A Venatus Geometry Point-Cloud Variational Auto-Encoder</div> <div class="author"> <em><b>Tejas Anvekar</b></em>,¬†Ramesh Ashok Tabib,¬†Dikshit Hegde,¬†and¬†Uma Mudengudi</div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</em>, Jun 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://openaccess.thecvf.com/content/CVPR2022W/DLGC/papers/Anvekar_VG-VAE_A_Venatus_Geometry_Point-Cloud_Variational_Auto-Encoder_CVPRW_2022_paper.pdf" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=vDF61_EMKBU" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>In this paper, we propose VG-VAE: Venatus Geometric Variational Auto-Encoder for capturing unsupervised hierarchical local and global geometric signatures in pointcloud. Recent research emphasises the significance of the underlying intrinsic geometry for pointcloud processing. Our contribution is to extract and analyse the morphology of the pointcloud using the proposed Geometric Proximity Correlator (GPC) and variational sampling of the latent. The extraction of local geometric signatures is facilitated by the GPC, whereas the extraction of global geometry is facilitated by variational sampling. Furthermore, we apply a naive mix of vector algebra and 3D geometry to extract the basic per-point geometric signature, which assists the unsupervised hypothesis. We provide statistical analyses of local and global geometric signatures. The impacts of our geometric features are demonstrated on pointcloud classification as downstream task using the classic pointcloud feature extractor PointNet. We demonstrate our analysis on ModelNet40 a benchmark dataset, and compare with state-of-the-art techniques. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Anvekar_2022_CVPR</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Anvekar, Tejas and Tabib, Ramesh Ashok and Hegde, Dikshit and Mudengudi, Uma}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{VG-VAE: A Venatus Geometry Point-Cloud Variational Auto-Encoder}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2978-2985}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/ASUR3D.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/ASUR3D.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/ASUR3D.gif-1400.webp"></source> <img src="/assets/img/publication_preview/ASUR3D.gif" class="preview z-depth-1 rounded" width="auto" height="auto" alt="ASUR3D.gif" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <abbr class="badge" style="background-color: #0076df;">ICCV-W</abbr> </div> <div id="Kumbar_2023_ICCV" class="col-sm-8"> <div class="title">ASUR3D: Arbitrary Scale Upsampling and Refinement of 3D Point Clouds Using Local Occupancy Fields</div> <div class="author"> Akash Kumbar,¬†<em><b>Tejas Anvekar</b></em>,¬†Ramesh Ashok Tabib,¬†and¬†Uma Mudenagudi</div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm" role="button">Abs</a> <a class="bibtex btn btn-sm" role="button">Bib</a> <a href="https://openaccess.thecvf.com/content/ICCV2023W/e-Heritage/papers/Kumbar_ASUR3D_Arbitrary_Scale_Upsampling_and_Refinement_of_3D_Point_Clouds_ICCVW_2023_paper.pdf" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=_fs3b8oMvKc" class="btn btn-sm" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p> In this paper, we introduce ASUR3D, a novel methodology for the arbitrary-scale upsampling of 3D point clouds employing Local Occupancy Representation. Our proposed implicit occupancy representation enables efficient point classification, effectively discerning points belonging to the surface from non-surface points. Learning an implicit representation of open surfaces, enables one to capture the better local neighbourhood representation, leading to finer refinement and reconstruction with enhanced preservation of intricate geometric details. Leveraging this capability, we can accurately sample an arbitrary number of points on the surface, facilitating precise and flexible upsampling. We demonstrate the effectiveness of ASUR3D on PUGAN and PU1K benchmark datasets. Our proposed method achieves state-of-the-art results on all benchmarks and for all evaluation metrics. Additionally, we demonstrate the efficacy of our methodology on self-proposed heritage data generated through photogrammetry, further confirming its effectiveness in diverse scenarios. The code is publicly available at https://github. com/Akash-Kumbar/ASUR3D</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Kumbar_2023_ICCV</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kumbar, Akash and Anvekar, Tejas and Tabib, Ramesh Ashok and Mudenagudi, Uma}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ASUR3D: Arbitrary Scale Upsampling and Refinement of 3D Point Clouds Using Local Occupancy Fields}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1652-1661}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <hr> <h2 class="font-weight">Reviewer Experience</h2> <div>DLGC | CVPR23-Workshop, ICRA24</div> </article> </div> <style>.social-icons{display:flex;justify-content:space-around;align-items:center;flex-wrap:wrap}.icon{flex:1 0 20%;max-width:100px;margin:3%;text-align:center}.icon img{width:10%;height:auto}.icon:hover{text-decoration:none;color:white;background-color:transparent!important}.icon:hover img{transform:translateY(-3px)}@media(max-width:768px){.icon{flex-basis:40%;max-width:80px}.icon img{width:7%;height:auto}}</style> <script type="module">
  import { annotate } from 'https://unpkg.com/rough-notation?module';
  
  // const isMobile = /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent);

  const n1 = document.querySelector('#yellow-highlight-1');
  const n2 = document.querySelector('#yellow-highlight-2');
  // const n3 = document.querySelector('#purple-highlight-1');
  const n4 = document.querySelector('#purple-highlight-2');
  const n5 = document.querySelector('#blue-highlight-1');
  const n6 = document.querySelector('#blue-highlight-2');
  // const n7 = document.querySelector('#blue-highlight-3');

  const a1 = annotate(n1, { type: 'highlight', color: 'yellow', iterations: 1 , multiline: true});
  const a2 = annotate(n2, { type: 'highlight', color: 'yellow', iterations: 1, multiline: true});
  // const a3 = annotate(n3, { type: 'highlight', color: '#F2A2E8'});
  const a4 = annotate(n4, { type: 'highlight', color: '#F2A2E8',iterations: 1, multiline: true});
  const a5 = annotate(n5, { type: 'highlight', color: '#ACE8FF',iterations: 1, multiline: true});
  const a6 = annotate(n6, { type: 'highlight', color: '#ACE8FF',iterations: 1, multiline: true});
  // const a7 = annotate(n7, { type: 'highlight', color: '#ACE8FF'});

  
//  if (!isMobile) {
  setTimeout(() => {
    a1.show();
    a2.show();
  }, 1000); // Delay in milliseconds
  
  // setTimeout(() => {
  //   a3.show();
  // }, 2000); // Delay in milliseconds
  
  setTimeout(() => {
    a4.show();
  }, 2000); // Delay in milliseconds
  
  setTimeout(() => {
    a5.show();
    a6.show();
  }, 3000); // Delay in milliseconds

  // setTimeout(() => {
  //   a7.show();
  // }, 4000); // Delay in milliseconds

// }

</script> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2023 Tejas Anvekar. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>